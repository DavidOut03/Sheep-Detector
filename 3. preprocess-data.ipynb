{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dac7a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: opencv-python in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: scikit-learn in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in s:\\deeplearning-inholland\\sheep-detector\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas opencv-python scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78bb24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbd8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./dataset\"\n",
    "orig_ann_file = os.path.join(data_folder, \"annotations.xlsx\")              \n",
    "aug_ann_file = os.path.join(data_folder, \"images\", \"augmented\", \"annotations.xlsx\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1084d1",
   "metadata": {},
   "source": [
    "# Load in annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132edd39",
   "metadata": {},
   "source": [
    "Before the images can be used by the yolo model the annotations and images still need to be preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4c3b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_excel(orig_ann_file)\n",
    "aug_df = pd.read_excel(aug_ann_file) if os.path.exists(aug_ann_file) else pd.DataFrame(columns=orig_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f7337a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 198 original and 585 augmented annotations, total = 783.\n"
     ]
    }
   ],
   "source": [
    "orig_df['source'] = 'original'\n",
    "aug_df['source'] = 'augmented'\n",
    "\n",
    "data = pd.concat([orig_df, aug_df], ignore_index=True)\n",
    "print(f\"Loaded {len(orig_df)} original and {len(aug_df)} augmented annotations, total = {len(data)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a5041",
   "metadata": {},
   "source": [
    "For the images to be able to be used by the yolo model the image annotations first need to be normalized. Also the class needs to be set to 0 because there is only one class sheep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a55b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class_id'] = 0\n",
    "data['x_center'] = data['bbox_x'] + data['bbox_width'] / 2.0\n",
    "data['y_center'] = data['bbox_y'] + data['bbox_height'] / 2.0\n",
    "data['width']    = data['bbox_width'].astype(float)\n",
    "data['height']   = data['bbox_height'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb5b9a",
   "metadata": {},
   "source": [
    "The yolo model can only work with 640x640 image but our images are 640x512 so the images need to be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_image_to_square(img: np.ndarray, target_size: int = 640, pad_color: Tuple[int,int,int] = (0,0,0)) -> np.ndarray:\n",
    "    \"\"\"Pad image to a square of target_size x target_size with padding color .\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    if h == target_size and w == target_size:\n",
    "        return img  # already square\n",
    "    # Calculate padding on each side\n",
    "    top = (target_size - h) // 2\n",
    "    bottom = target_size - h - top\n",
    "    left = (target_size - w) // 2\n",
    "    right = target_size - w - left\n",
    "    padded_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=pad_color)\n",
    "    return padded_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2cddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_width = 640\n",
    "original_height = 512\n",
    "target_size = 640\n",
    "\n",
    "top_padding = (target_size - original_height) // 2\n",
    "left_padding = (target_size - original_width) // 2\n",
    "\n",
    "# Compute adjusted center coordinates (in padded image coordinates)\n",
    "data['x_center'] = (data['x_center']) + left_padding\n",
    "data['y_center'] = (data['y_center']) + top_padding\n",
    "\n",
    "# Normalize to 640x640 (after padding)\n",
    "data['x_center'] = data['x_center'] / target_size\n",
    "data['y_center'] = data['y_center'] / target_size\n",
    "data['width'] = data['width'] / target_size\n",
    "data['height'] = data['height'] / target_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ee052d",
   "metadata": {},
   "source": [
    "Now that the dataset is ready we can split it up to a training, validation and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec523b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split images: split\n",
      "train    558\n",
      "val      122\n",
      "test     103\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_images = data['image_name'].unique()\n",
    "train_imgs, temp_imgs = train_test_split(unique_images, test_size=0.3, random_state=42)   # 70% train, 30% temp\n",
    "val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)         # split remaining 30% equally into val/test\n",
    "\n",
    "def assign_split(image_name: str) -> str:\n",
    "    if image_name in train_imgs:\n",
    "        return 'train'\n",
    "    elif image_name in val_imgs:\n",
    "        return 'val'\n",
    "    else:\n",
    "        return 'test'\n",
    "\n",
    "data['split'] = data['image_name'].apply(assign_split)\n",
    "print(\"Split images:\", data['split'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4688084",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train', 'val', 'test']\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(data_folder, 'images', split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(data_folder, 'labels', split), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b0f37e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset images and labels have been exported to train/val/test splits.\n"
     ]
    }
   ],
   "source": [
    "for (image_name, split), group in data.groupby(['image_name', 'split']):\n",
    "    # Determine source folder (enhanced for original images, generated for augmented images)\n",
    "    source_folder = \"augmented\" if group['source'].iloc[0] == 'augmented' else \"enhanced\"\n",
    "    src_path = os.path.join(data_folder, \"images\", source_folder, image_name)\n",
    "    dst_img_path = os.path.join(data_folder, \"images\", split, image_name)\n",
    "    dst_lbl_path = os.path.join(data_folder, \"labels\", split, os.path.splitext(image_name)[0] + \".txt\")\n",
    "\n",
    "    if not os.path.exists(src_path):\n",
    "        print(f\"⚠️ Source image not found: {src_path} (skipping)\")\n",
    "        continue\n",
    "\n",
    "    # Read image and pad to square\n",
    "    img = cv2.imread(src_path)\n",
    "    padded_img = pad_image_to_square(img, target_size=640)\n",
    "    cv2.imwrite(dst_img_path, padded_img)\n",
    "\n",
    "    # Write label file in YOLO format\n",
    "    with open(dst_lbl_path, 'w') as f:\n",
    "        for _, row in group.iterrows():\n",
    "            class_id = int(row['class_id'])\n",
    "            x_c, y_c, w, h = row['x_center'], row['y_center'], row['width'], row['height']\n",
    "            f.write(f\"{class_id} {x_c:.6f} {y_c:.6f} {w:.6f} {h:.6f}\\n\")\n",
    "print(\"✅ Dataset images and labels have been exported to train/val/test splits.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
